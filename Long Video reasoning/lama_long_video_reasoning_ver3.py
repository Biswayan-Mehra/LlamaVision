# -*- coding: utf-8 -*-
"""Lama long video reasoning ver3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HV2iq_Z6pOGfxj1KzguFSZ4QFFCtt8H-

## Get started
"""

!pip install yt-dlp

! pip3 install --upgrade --quiet google-cloud-aiplatform openai

"""### Restart runtime (Colab only)

To use the newly installed packages, you must restart the runtime on Google Colab.
"""

import sys

if "google.colab" in sys.modules:

    import IPython

    app = IPython.Application.instance()
    app.kernel.do_shutdown(True)

"""<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>
</div>

### Authenticate your notebook environment (Colab only)

Authenticate your environment on Google Colab.
"""

import sys

if "google.colab" in sys.modules:

    from google.colab import auth

    auth.authenticate_user()

PROJECT_ID = "ecstatic-moon-439010-e7"  # @param {type:"string"}

# Only `us-central1` is supported region for Llama 3.2 models using Model-as-a-Service (MaaS).
LOCATION = "us-central1"

BUCKET_NAME = "karunyabucket501"  # @param {type:"string"}

BUCKET_URI = f"gs://{BUCKET_NAME}"

"""**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."""

#! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}

"""### Initialize Vertex AI SDK for Python"""

import vertexai

vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)

"""### Import libraries

Import libraries to use in this tutorial.
"""

import json
import re
import uuid
from io import BytesIO

#import gradio as gr
import matplotlib.pyplot as plt
# Chat completions API
import openai
from google.auth import default, transport
from google.cloud import storage
from PIL import Image

"""### Helpers functions"""

def visualize_image_from_bucket(bucket_name: str, blob_name: str) -> None:
    """Visualizes an image stored in a Google Cloud Storage bucket."""
    try:
        # Create a client for interacting with Google Cloud Storage
        storage_client = storage.Client()

        # Get a reference to the bucket and blob
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)

        # Download the image data into memory
        image_data = blob.download_as_bytes()

        # Open the image using PIL
        image = Image.open(BytesIO(image_data))

        # Display the image using matplotlib
        plt.figure(figsize=(10, 10))  # Set the figure size (adjust as needed)
        plt.imshow(image)
        plt.axis("off")  # Turn off axis labels
        plt.show()

    except Exception as e:
        print(f"Error visualizing image: {e}")

"""#### Authentication

You can request an access token from the default credentials for the current environment. Note that the access token lives for [1 hour by default](https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.

"""

credentials, _ = default()
auth_request = transport.requests.Request()
credentials.refresh(auth_request)

"""Then configure the  SDK to point to the Llama 3.2 Chat Completions API endpoint.

Notice, only `us-central1` is supported region for Llama 3.2 models using Model-as-a-Service (MaaS).
"""

MODEL_LOCATION = "us-central1"
MAAS_ENDPOINT = f"{MODEL_LOCATION}-aiplatform.googleapis.com"

client = openai.OpenAI(
    base_url=f"https://{MAAS_ENDPOINT}/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi",
    api_key=credentials.token,
)

"""#### Llama 3.2 models

You can experiment with various supported Llama 3.2 models.

This tutorial use Llama 3.2 90B Vision Instruct using Model-as-a-Service (MaaS). Using Model-as-a-Service (MaaS), you can access Llama 3.2 models in just a few clicks without any setup or infrastructure hassles.

You can also access Llama models for self-service in Vertex AI Model Garden, allowing you to choose your preferred infrastructure. [Check out Llama 3.2 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3-2?_ga=2.31261500.2048242469.1721714335-1107467625.1721655511) to learn how to deploy a Llama 3.2 models on Vertex AI.
"""

MODEL_ID = "meta/llama-3.2-90b-vision-instruct-maas"  # @param {type:"string"} ["meta/llama-3.2-90b-vision-instruct-maas"]

"""### Chat with Llama 3.2

Use the Chat Completions API to send a multi-model request to the Llama 3.2 model.

#### Hello, Llama 3.2
"""

max_tokens = 4096

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "image_url": {
                        "url": "gs://github-repo/img/gemini/intro/landmark1.jpg"
                    },
                    "type": "image_url",
                },
                {"text": "What’s in this image?", "type": "text"},
            ],
        },
        {"role": "assistant", "content": "In this image, you have:"},
    ],
    max_tokens=max_tokens,
)

"""You get the response as shown below."""

print(response.choices[0].message.content)

"""You use the helper function to visualize the image."""

def upload_image_to_bucket(image_path, bucket_name, blob_name):
    """Uploads an image to a Google Cloud Storage bucket."""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(image_path)
    image_uri = f"gs://{bucket_name}/{blob_name}"
    return image_uri

"""# module 2"""

from IPython.display import clear_output

class CFG:

    ### model
    model_name = "meta-llama/Llama-3.2-11B-Vision-Instruct"
    max_tokens = 2560
    temperature = 0.01

    ### input data
    video_url = "https://www.youtube.com/watch?v=F9Ea1YfoEW4"
    video_url = "https://www.youtube.com/watch?v=JyNslOkVBVo"

    ### output dir
    output_path = "./"

import yt_dlp

def download_youtube_video(video_url, save_path='.'):
    ydl_opts = {
        'outtmpl': f'{save_path}/%(title)s.%(ext)s',  # Save with video title and extension
        'format': 'best'  # Download the best available quality
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(video_url, download=True)


def download_youtube_videoNEW(video_url, save_path='.', videofilename='1.mp4'):
    ydl_opts = {
        'outtmpl': f'{save_path}/{videofilename}',  # Save with video title and extension
        'format': 'best'  # Download the best available quality
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(video_url, download=True)

import os

#video_properties["frame_count"]/video_properties["duration_seconds"]



def download_youtube_videoNEW(video_url, save_path='.', videofilename='1.mp4'):
    ydl_opts = {
        'outtmpl': f'{save_path}/{videofilename}',  # Save with video title and extension
        'format': 'best'  # Download the best available quality
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(video_url, download=True)

#download_youtube_videoNEW(CFG.video_url)

import os
import glob
import gc
import subprocess

from PIL import Image
import cv2

import torch

def get_video_properties(video_path):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    # Check if the video file is opened successfully
    if not cap.isOpened():
        raise ValueError("Could not open video file")

    # Get video properties
    properties = {
        "fps": int(cap.get(cv2.CAP_PROP_FPS)),
        "frame_count": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
        "duration_seconds": int( cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS) ),
        "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
        "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
        "codec": int(cap.get(cv2.CAP_PROP_FOURCC)),
    }

    # Release the video capture object
    cap.release()

    return properties

print(CFG.output_path)

os.path.join("", '*.mp4')

#filename = glob.glob(os.path.join("/content/", '*.mp4'))[2]

filename = "/content/1.mp4"

print(filename)

INPUT_VIDEO = filename

print(INPUT_VIDEO)



OUT_VIDEO_NAME = filename.split('/')[0] + '/new_codec_' + filename.split('/')[-1]

print(OUT_VIDEO_NAME)

def grab_frames(video_path, frames_per_second):
    # Open the video file or stream
    video = cv2.VideoCapture(video_path)

    # Get the frame rate of the video
    fps = video.get(cv2.CAP_PROP_FPS)
    print(f'Video FPS: {fps}\n')

    # Calculate the interval between frames to capture
    frame_interval = int(fps // frames_per_second)

    frames = []
    frame_count = 0

    while True:
        ret, frame = video.read()

        if not ret:
            break  # Exit if no more frames

        # Capture a frame every 'frame_interval'
        if frame_count % frame_interval == 0:
            # Convert frame to PIL image (OpenCV reads in BGR format, so convert to RGB)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            frames.append(pil_image)

        frame_count += 1

    # Release the video capture object
    video.release()

    return frames

from IPython.display import clear_output
from IPython.display import Video

VideoURL = "https://www.youtube.com/watch?v=rSpHmNQc1nA" #@param {type:"string"}
Question = "Briefly describe what is happening in this sequence of image frames:" #@param {type:"string"}
FPS = 17  #@param {type: "slider", min: 1, max: 100}
LLMBundle = 8  #@param {type: "slider", min: 1, max: 10}

# @title
JoinofAllSections = ""
JoinofAllSectionsChapterWise = " SECTION 1. "
SetofClips = []
myuserquestion = Question

file_path = "/content/1.mp4"
if os.path.exists(file_path):
    os.remove(file_path)

download_youtube_videoNEW(VideoURL)
filename = "/content/1.mp4"

video_properties = get_video_properties(video_path = filename)
INPUT_VIDEO = filename

print(INPUT_VIDEO)

framesTotal = grab_frames(INPUT_VIDEO, frames_per_second=FPS/100)

noofframesinaclip = (int) (len(framesTotal)/LLMBundle )

print(noofframesinaclip)

for i in range(LLMBundle):
  startindex = i * noofframesinaclip
  mygroup = framesTotal[startindex:startindex+noofframesinaclip]
  print(len(mygroup))

  frames = mygroup

  print("no of frames")
  print(len(frames))

  total_width = sum(frame.size[0] for frame in frames)
  max_height = max(frame.size[1] for frame in frames)

  ### Create a new blank image with the combined width and maximum height
  concatenated_image = Image.new('RGB', (total_width, max_height))

  ### Paste each image side by side
  x_offset = 0
  for frame in frames:
      concatenated_image.paste(frame, (x_offset, 0))
      x_offset += frame.size[0]



  concatenated_image.save("myFrames.png")
  SetofClips.append(concatenated_image)

  image_path = "/content/myFrames.png"

  bucket_name = BUCKET_NAME
  blob_name = "your-image-name.jpg"
  image_uri = upload_image_to_bucket(image_path, bucket_name, blob_name)
  print(f"Image uploaded to: {image_uri}")

  #myuserquestion= "Briefly describe what is happening in this sequence of images:"

  max_tokens = 4000

  response = client.chat.completions.create(
      model=MODEL_ID,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "image_url": {
                          "url": "gs://karunyabucket501/your-image-name.jpg"
                      },
                      "type": "image_url",
                  },
                  {"text": myuserquestion, "type": "text"},
              ],
          },
          {"role": "assistant", "content": "  "},
      ],
      max_tokens=max_tokens,
  )

  print(response.choices[0].message.content)

  if(type(response.choices[0].message.content) == str):
    JoinofAllSections =  JoinofAllSections + " " + response.choices[0].message.content
    JoinofAllSectionsChapterWise = JoinofAllSectionsChapterWise + " SECTION " + str(i) + " . " + response.choices[0].message.content

  display(concatenated_image)

print(JoinofAllSections)

JoinofAllSections

AskLongVideo = "Describe the Brain gym activity" #@param {type:"string"}

# @title
prompt2 = "Question: " + AskLongVideo +   ". Context follows: " + JoinofAllSections

response2 = client.chat.completions.create(
      model=MODEL_ID,
      messages=[
          {
              "role": "user",
              "content": [

                  {"text": prompt2, "type": "text"},
              ],
          },
          {"role": "assistant", "content": "Answer: "},
      ],
      max_tokens=max_tokens,
  )

print(response2.choices[0].message.content)


prompt3 = "In which section the answer is present? Just output the Section Number only as single digit output." + "Question: " + AskLongVideo +   ". Context follows: " + JoinofAllSectionsChapterWise
response3 = client.chat.completions.create(
      model=MODEL_ID,
      messages=[
          {
              "role": "user",
              "content": [

                  {"text": prompt3, "type": "text"},
              ],
          },
          {"role": "assistant", "content": " Answer is found in Section  "},
      ],
      max_tokens=max_tokens,
  )
print(response3.choices[0].message.content)

if(len(response3.choices[0].message.content)==1):
  try:
    gettinganumber = (int) (response3.choices[0].message.content)
    print(gettinganumber)
    display(SetofClips[gettinganumber])
  except ZeroDivisionError:
    print("")

YouTubeDataIDkey = "Enter your youTube API Key"

!gcloud projects add-iam-policy-binding ecstatic-moon-439010-e7 \
    --member="user:rajagopal.motivate@gmail.com" \
    --role="roles/aiplatform.user"

!gcloud projects add-iam-policy-binding ecstatic-moon-439010-e7 \
    --member="user:biswayanmehra@karunya.edu.in" \
    --role="roles/aiplatform.user"

!gcloud storage buckets add-iam-policy-binding gs://coolbucket500 --member="user:rajagopal.motivate@gmail.com" --role="roles/storage.objectCreator"

!gcloud storage buckets add-iam-policy-binding gs://coolbucket500 \
    --member="user:rajagopal.motivate@gmail.com" \
    --role="roles/storage.objectAdmin"

!gcloud storage buckets add-iam-policy-binding gs://coolbucket500 \
    --member="user:biswayanmehra@karunya.edu.in" \
    --role="roles/storage.objectAdmin"

!gcloud storage buckets add-iam-policy-binding gs://coolbucket500 \
    --member="user:biswayanmehra@karunya.edu.in" \
    --role="roles/storage.objectCreator"

!gcloud storage buckets add-iam-policy-binding "coolbucket500"\
    --member="user:biswayanmehra@karunya.edu.in"\
    --role="roles/storage.objectCreator"

!pip install google-cloud-storage

import os
from google.cloud import storage

# Explicitly set the GOOGLE_APPLICATION_CREDENTIALS environment variable
# Replace 'path/to/your/credentials.json' with the actual path to your credentials file
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your/credentials.json'

def upload_image_to_bucket(image_path, bucket_name, blob_name):
    """Uploads an image to a Google Cloud Storage bucket."""
    # Initialize the storage client using the environment variable
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(image_path)
    image_uri = f"gs://{bucket_name}/{blob_name}"
    return image_uri

# Usage:
image_path = "path/to/your/image.jpg"
image_path = "/content/sunset.jpg"

bucket_name = "coolbucket500"
blob_name = "your-image-name.jpg"
image_uri = upload_image_to_bucket(image_path, bucket_name, blob_name)
print(f"Image uploaded to: {image_uri}")

"""#### Ask Llama 3.2 using different model configuration

Use the following parameters to generate different answers:

*   `temperature` to control the randomness of the response
*   `top_p` to control the quality of the response
*   `stream` to stream the response back or not

"""

temperature = 1.0  # @param {type:"number"}
top_p = 1.0  # @param {type:"number"}
stream = True  # @param {type:"boolean"}

"""Get the answer."""

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "image_url": {
                        "url": "gs://github-repo/img/gemini/intro/landmark2.jpg"
                    },
                    "type": "image_url",
                },
                {"text": "What’s in this image?", "type": "text"},
            ],
        },
        {"role": "assistant", "content": "In this image, you have:"},
    ],
    temperature=temperature,
    max_tokens=max_tokens,
    top_p=top_p,
    stream=stream,
)

"""Depending if `stream` parameter is enabled or not, you can print the response entirely or chunk by chunk."""

if stream:
    for chunk in response:
        print(chunk.choices[0].delta.content, end="")
else:
    print(response.choices[0].message.content)

"""And again, let's check if the answer is correct."""

visualize_image_from_bucket("github-repo", "img/gemini/intro/landmark2.jpg")

"""#### Use Llama 3.2 with different multimodal tasks

In this section, you will use Llama 3.2 to perform different multimodal tasks including image captioning and Visual Question Answering (VQA).

For each task, you'll define a different prompt and submit a request to the model as you did before.
"""

visualize_image_from_bucket("github-repo", "img/gemini/intro/landmark3.jpg")

"""##### Image captioning"""

prompt = "Imagine you're telling a friend about this photo. What would you say?"

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "image_url": {
                        "url": "gs://github-repo/img/gemini/intro/landmark3.jpg"
                    },
                    "type": "image_url",
                },
                {"text": prompt, "type": "text"},
            ],
        },
    ],
    max_tokens=max_tokens,
)

print(response.choices[0].message.content)

"""##### Visual Question Answering (VQA)"""

prompt = """
Analyze this image and answer the following questions:
- What is the primary color in the image?
- What is the overall mood or atmosphere conveyed in the scene?
- Based on the visual clues, who might have taken the picture?"
"""

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "image_url": {
                        "url": "gs://github-repo/img/gemini/intro/landmark3.jpg"
                    },
                    "type": "image_url",
                },
                {"text": prompt, "type": "text"},
            ],
        },
    ],
    max_tokens=max_tokens,
)

print(response.choices[0].message.content)

# Create a client for interacting with Google Cloud Storage
storage_client = storage.Client()

     # Get a reference to the bucket
bucket = storage_client.bucket(BUCKET_NAME)


        # Get a reference to the blob and upload the image
blob = bucket.blob("myimage1")
blob.upload_from_filename(image_path)

        # Construct the URI of the uploaded image
image_uri = f"gs://{BUCKET_NAME}/{blob_name}"

print(response.choices[0].message.content)



"""### Build with Llama 3.2 : Car Damage Assessment app using Gradio

In this section, you use Llama 3.2 to build a simple GenAI powered application for Car Damage Assessment.

In this scenario, the app has to cover the following tasks:

*   Classify the type of damage
*   Estimate the damage severity
*   Estimate the damage cost

#### Define the UI functions
"""

def upload_image_to_bucket(image_path: str) -> str:
    """Uploads an image to a Google Cloud Storage bucket."""
    try:
        # Create a client for interacting with Google Cloud Storage
        storage_client = storage.Client()

        # Get a reference to the bucket
        bucket = storage_client.bucket(BUCKET_NAME)

        # Generate a unique blob name based on the file extension
        file_extension = image_path.split(".")[-1].lower()
        if file_extension in ["jpg", "jpeg"]:
            blob_name = f"car_damage_{uuid.uuid4()}.jpg"
        else:
            blob_name = f"car_damage_{uuid.uuid4()}.png"

        # Get a reference to the blob and upload the image
        blob = bucket.blob(blob_name)
        blob.upload_from_filename(image_path)

        # Construct the URI of the uploaded image
        image_uri = f"gs://{BUCKET_NAME}/{blob_name}"
        return image_uri

    except Exception as e:
        print(f"Error uploading image: {e}")


def parse_json_from_markdown(markdown_text: str) -> dict | None:
    """Extracts and parses JSON content embedded within Markdown text."""
    json_pattern = r"```json\n(.*?)\n```"
    match = re.search(json_pattern, markdown_text, re.DOTALL)

    if match:
        json_content = match.group(1)
        try:
            parsed_data = json.loads(json_content)
            return parsed_data
        except json.JSONDecodeError as e:
            print(f"Error: Invalid JSON content found. {e}")
            return None
    else:
        return None


def process_image(image_uri):
    """Processes a car damage image using a multimodal LLM."""

    # Construct the prompt
    prompt = """
    Analyze the provided image of a car and provide the following information:

    1. Damage Type: Identify the primary type of damage visible in the image (e.g., dent, scratch, cracked windshield, etc.).
    2. Severity: Estimate the severity of the damage on a scale of 1 to 5, where 1 is minor and 5 is severe.
    3. Estimated Repair Cost: Provide an approximate range for the repair cost in USD.

    Return the results in JSON format with damagetype, severity, and cost fields.
    """

    # Call Llama model
    credentials, _ = default()
    auth_request = transport.requests.Request()
    credentials.refresh(auth_request)

    client = openai.OpenAI(
        base_url=f"https://{MAAS_ENDPOINT}/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi",
        api_key=credentials.token,
    )
    response = client.chat.completions.create(
        model=MODEL_ID,
        messages=[
            {
                "role": "user",
                "content": [
                    {"image_url": {"url": image_uri}, "type": "image_url"},
                    {"text": prompt, "type": "text"},
                ],
            },
        ],
        max_tokens=max_tokens,
    )

    # Parse the response
    response = response.choices[0].message.content
    output = parse_json_from_markdown(response)

    output = {"damagetype": "scratch", "severity": 5, "cost": 1000}
    return output["damagetype"], output["severity"], output["cost"]


def demo_fn(image_path):
    """
    Processes a car damage image using a multimodal LLM.
    """

    # Upload the image
    image_uri = upload_image_to_bucket(image_path)

    # Process the image
    damagetype, severity, cost = process_image(image_uri)

    return damagetype, severity, cost

"""#### Run the application"""

demo = gr.Interface(
    fn=demo_fn,
    inputs=gr.Image(type="filepath"),
    outputs=[
        gr.Textbox(label="Damage Type"),
        gr.Slider(label="Severity", minimum=1, maximum=10, step=1),
        gr.Number(label="Cost"),
    ],
    title="Car Damage Assessment",
)

demo.launch(debug=True)

demo.close()

"""## Cleaning up

Clean up resources created in this notebook.

To delete to the search engine in Vertex AI, check out the following [documentation](https://cloud.google.com/generative-ai-app-builder/docs/delete-engine).
"""

delete_bucket = False  # @param {type:"boolean"}

if delete_bucket:
    ! gsutil -m rm -r $BUCKET_NAME
